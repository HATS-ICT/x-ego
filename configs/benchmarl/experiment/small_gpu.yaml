defaults:
  - experiment_config
  - _self_

# Low-memory experiment config (minimize CUDA memory usage).
# Notes:
# - Puts sampling + buffer on CPU to keep CUDA memory minimal.
# - Uses smallest possible on-policy batch/minibatch sizes.

# Devices
sampling_device: "cpu"
train_device: "cuda"
buffer_device: "cpu"

# Policy + collection
share_policy_params: True
prefer_continuous_actions: True
collect_with_grad: False
parallel_collection: False

# Optimizer
gamma: 0.99
lr: 0.00005
adam_eps: 0.000001
adam_extra_kwargs: {}
clip_grad_norm: True
clip_grad_val: 5

# Target updates
soft_target_update: True
polyak_tau: 0.005
hard_target_update_frequency: 5

# Exploration
exploration_eps_init: 0.8
exploration_eps_end: 0.01
exploration_anneal_frames: null

# Termination
max_n_iters: null
max_n_frames: 3_000_000

# On-policy collection (low memory)
on_policy_collected_frames_per_batch: 1
on_policy_n_envs_per_worker: 1
on_policy_n_minibatch_iters: 1
on_policy_minibatch_size: 1

# Off-policy defaults (unused for MAPPO; set small to minimize memory if you switch algos)
off_policy_collected_frames_per_batch: 1
off_policy_n_envs_per_worker: 1
off_policy_n_optimizer_steps: 1
off_policy_train_batch_size: 1
off_policy_memory_size: 10_000
off_policy_init_random_frames: 0
off_policy_use_prioritized_replay_buffer: False
off_policy_prb_alpha: 0.6
off_policy_prb_beta: 0.4

# Evaluation/logging (disabled for speed)
evaluation: True
render: True
evaluation_interval: 120_000
evaluation_episodes: 10
evaluation_deterministic_actions: True
evaluation_static: False

loggers: [wandb, csv]
project_name: "benchmarl"
wandb_extra_kwargs: {}
create_json: True

save_folder: "./results"
restore_file: null
restore_map_location: null
checkpoint_interval: 120_000
checkpoint_at_end: True
keep_checkpoints_num: 3
exclude_buffer_from_checkpoint: False


