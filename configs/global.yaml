meta:
  seed: 42

data:
  partition: all # train, val, test, all
  labels_folder: labels
  video_folder: video_544x306_30fps
  persistent_workers: true
  pin_mem: true
  batch_size: 16
  num_workers: 16
  prefetch_factor: 2
  data_path_csv_filename: video_partitioned_chunked_downsized.csv
  fixed_duration_seconds: 5
  target_fps: 4
  video_column_name: video_path
  video_processor_model: facebook/vjepa2-vitl-fpc16-256-ssv2
  video_size_mode: resize_distort
  num_pov_agents: 5  # Number of agents from team side to use for inference (1-5, recommend 5 for full team)
                     # When contrastive learning is enabled, dataset always samples 5 agents,
                     # then model randomly selects num_pov_agents from these 5 for inference
  mask_minimap: true
  time_jitter_max_seconds: 0.3  # Maximum random time jitter (±seconds) applied to each sampled frame for temporal augmentation. Set to 0 to disable.
  grid_resolution: 100  # Grid resolution (10x10 = 100 cells)
  gaussian_sigma: 2.0  # Gaussian smoothing sigma for density-cls

  
model:
  activation: gelu
  num_pov_agents: ${data.num_pov_agents}
  num_target_agents: 5
  team_embed_dim: 32
  hidden_dim: 256
  num_hidden_layers: 3
  dropout: 0.1
  class_weights: null  # Options: null, inverse, inverse_sqrt, effective_num, pos_weight
  agent_fusion:
    method: concat  # Options: "mean", "max", "attention", "concat"
    fused_agent_dim: 768
    num_attn_heads: 8 # only when method is attention
    num_layers: 2
  vae:
    latent_dim: 64
    kl_weight: 0.5
  loss_fn:
    multi-label-cls: bce  # Options: bce, focal
    grid-cls: bce  # Options: bce, focal
    multi-output-reg: mse  # Options: mse, mae
    density-cls: mse  # Options: mse, mae, kl
    coord-reg: sinkhorn  # Options: mse, sinkhorn, hausdorff, energy
    coord-gen: sinkhorn  # Options: mse, sinkhorn, hausdorff, energy
    traj-gen: mse  # Options: mse, smooth_l1, huber
  sinkhorn:
    blur: 0.1  # Controls smoothing (0.01-0.1)
    scaling: 0.8  # Speeds convergence with ε-scaling (<1)
    p: 2  # Power parameter for Sinkhorn loss
  hausdorff:
    p: 2  # Power parameter for Hausdorff loss
  energy:
    p: 2  # Power parameter for Energy loss
  focal:
    alpha: 0.25  # Class balance weight for positive class
    gamma: 2.0  # Focusing parameter (higher = more focus on hard examples)
  encoder:
    video:
      freeze_backbone: true
      model_type: dinov2  # Options: clip, dinov2, siglip, vivit, videomae, vjepa2
    proj_dim: ${model.agent_fusion.fused_agent_dim}
  contrastive:
    enable: false
    logit_scale_init: 1.0  # Temperature parameter (pass actual value, not log)
    logit_bias_init: 0.0   # Bias parameter (pass actual value)
    turn_off_bias: false   # If true, no bias is applied (for SigLIP ablation setup 1: b=n/a)
    # SigLIP ablation configurations:
    # Setup 1: logit_scale_init=10, logit_bias_init=0,   turn_off_bias=true  (b: n/a,  t: log10)
    # Setup 2: logit_scale_init=10, logit_bias_init=-10, turn_off_bias=false (b: -10,  t: log10)
    # Setup 3: logit_scale_init=1,  logit_bias_init=-10, turn_off_bias=false (b: -10,  t: log1)
    # Setup 4: logit_scale_init=10, logit_bias_init=0,   turn_off_bias=false (b: 0,    t: log10)
    # Setup 5: logit_scale_init=1,  logit_bias_init=0,   turn_off_bias=false (b: 0,    t: log1)
    loss_weight: 1.0
  traj_decoder:
    feat_dim: ${model.agent_fusion.fused_agent_dim}
    seq_len: 10
    num_teams: 2
    team_emb_dim: ${model.team_embed_dim}
    num_heads: 8
    num_layers: 2
    mlp_ratio: 4.0
    dropout: 0.1


training:
  max_epochs: 8  # Total training epochs
  max_steps: -1  # Disabled when using max_epochs
  accelerator: gpu
  devices: 1
  strategy: auto
  precision: bf16-mixed
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2
  val_check_interval: null  # Disabled when using check_val_every_n_epoch
  check_val_every_n_epoch: 1  # Validate at the end of each epoch
  log_every_n_steps: 10
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  torch_compile: false
  deterministic: false
  num_sanity_val_steps: 0
  limit_train_batches: 1.0 # 1.0 means use all data
  limit_val_batches: 1.0
  limit_test_batches: 1.0


optimization:
  lr: 0.0003
  weight_decay: 0.01
  fused_optimizer: true  # Enable fused AdamW and disable gradient clipping

wandb:
  enabled: true
  project: xego

checkpoint:
  epoch:
    filename: ${meta.run_name}-e{epoch:02d}-s{step:06d}-l{val/loss:.4f}
    monitor: 'val/loss'
    mode: min
    save_top_k: 1  # Save the best checkpoint based on validation loss
    save_last: true  # Also save the last checkpoint
    auto_insert_metric_name: false
    save_on_train_epoch_end: true
    every_n_epochs: 1  # Checkpoint every epoch
  step:
    filename: ${meta.run_name}-s{step:06d}-e{epoch:02d}
    monitor: null
    mode: min
    save_top_k: 0  # Disabled: don't save step-based checkpoints
    save_last: false
    auto_insert_metric_name: false
    save_on_train_epoch_end: false
    every_n_train_steps: null  # Disabled: step-based checkpointing turned off
